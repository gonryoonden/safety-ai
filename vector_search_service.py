import json
import os
import logging
import faiss
import numpy as np
import google.generativeai as genai
import requests
import time
import re
from typing import List, Dict, Any, Tuple
from typing import List, Dict, Any, Tuple, Optional # Optional ì¶”ê°€
from concurrent.futures import ThreadPoolExecutor, as_completed # ğŸ‘ˆ ì´ ì¤„ì„ ì¶”ê°€í•˜ì„¸ìš”.

# --- ìƒìˆ˜ ---
RAW_DATA_FILE = "law_database.json"
KNOWLEDGE_BASE_FILE = "answers.json"
EMBEDDING_MODEL = "models/text-embedding-004"
FAISS_INDEX_FILE = "faiss_index.bin"
ID_MAP_FILE = "faiss_id_map.json"
LAW_API_OC = os.getenv("LAW_API_OC", "test")

# --- ì „ì—­ ë³€ìˆ˜ ---
index = None
id_map = None
knowledge_base = None

# --- ë¡œê¹… ì„¤ì • ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [%(levelname)s] - %(message)s')

# ==================================================================================
# â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ í•µì‹¬ ìˆ˜ì • ì˜ì—­ START â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
# ==================================================================================

def format_jo_no(jo_no_str: str) -> str:
    """ì¡°ë¬¸ ë²ˆí˜¸ë¥¼ APIê°€ ìš”êµ¬í•˜ëŠ” 6ìë¦¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. (ì˜ˆ: '2' -> '000200', '4ì˜2' -> '000402')"""
    parts = jo_no_str.split('ì˜')
    main_no = parts[0].zfill(4)
    sub_no = parts[1].zfill(2) if len(parts) > 1 else "00"
    return main_no + sub_no

def flatten_api_response(article_detail: Dict[str, Any]) -> str:
    """
    ë²•ë ¹ ë³¸ë¬¸ APIì˜ ê³„ì¸µì  JSON ë°ì´í„°ë¥¼ ì¬ê·€ì ìœ¼ë¡œ íŒŒì‹±í•˜ì—¬,
    ì¡°(æ¢), í•­(é …), í˜¸(è™Ÿ), ëª©(ç›®)ê¹Œì§€ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    (ë“¤ì—¬ì“°ê¸° ê°•í™” ìµœì¢… ë²„ì „)
    """

    def ensure_list(obj):
        if obj is None: return []
        if isinstance(obj, list): return obj
        return [obj]

    def parse_mok(mok_list):
        result = []
        for mok in ensure_list(mok_list):
            mok_content = mok.get('ëª©ë‚´ìš©', '').strip()
            if mok_content:
                # ëª©(ç›®)ì€ 4ì¹¸ ë“¤ì—¬ì“°ê¸°
                result.append(f"    {mok_content}")
        return result

    def parse_ho(ho_list):
        result = []
        for ho in ensure_list(ho_list):
            ho_content = ho.get('í˜¸ë‚´ìš©', '').strip()
            line_parts = []
            if ho_content:
                # í˜¸(è™Ÿ)ëŠ” 2ì¹¸ ë“¤ì—¬ì“°ê¸°
                line_parts.append(f"  {ho_content}")

            mok_items = ho.get('ëª©')
            if mok_items:
                mok_lines = parse_mok(mok_items)
                if mok_lines:
                    line_parts.append('\n'.join(mok_lines))

            if line_parts:
                result.append('\n'.join(line_parts))
        return result

    def parse_hang(hang_list):
        result = []
        for hang in ensure_list(hang_list):
            hang_content = hang.get('í•­ë‚´ìš©', '').strip()
            line_parts = []
            if hang_content:
                line_parts.append(hang_content)

            ho_items = hang.get('í˜¸')
            if ho_items:
                ho_lines = parse_ho(ho_items)
                if ho_lines:
                    line_parts.append('\n'.join(ho_lines))

            if line_parts:
                result.append('\n'.join(line_parts))
        return result

    # --- ì¡°ë¬¸(æ¢) ê³„ì¸µ ì²˜ë¦¬ ---
    jo_no = article_detail.get('ì¡°ë¬¸ë²ˆí˜¸', '')
    jo_title = article_detail.get('ì¡°ë¬¸ì œëª©', '')
    jo_content = article_detail.get('ì¡°ë¬¸ë‚´ìš©', '').strip()

    result_lines = []

    if jo_content:
        result_lines.append(jo_content)
    else:
        jo_prefix = f"ì œ{jo_no}ì¡°"
        if jo_title:
            jo_prefix += f"({jo_title})"
        result_lines.append(jo_prefix)

    hang_items = article_detail.get('í•­')
    if hang_items:
        hang_lines = parse_hang(hang_items)
        if hang_lines:
            result_lines.append('\n'.join(hang_lines))

    return '\n'.join(result_lines)

def fetch_law_details(law_mst: str) -> List[Dict[str, Any]]:
    """
    ë²•ë ¹ì¼ë ¨ë²ˆí˜¸(law_mst)ì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  ì¡°ë¬¸ì˜ ìƒì„¸ ë‚´ìš©ì„ ë³‘ë ¬ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    list_api_url = f"http://www.law.go.kr/DRF/lawService.do?OC={LAW_API_OC}&target=jo&MST={law_mst}&type=JSON"

    try:
        logging.info(f"'{law_mst}'ì— ëŒ€í•œ ì¡°ë¬¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        list_response = requests.get(list_api_url, timeout=15)
        list_response.raise_for_status()
        list_data = list_response.json()
        jo_list = list_data.get("Jo", [])
    except (requests.RequestException, json.JSONDecodeError) as e:
        logging.error(f"'{law_mst}'ì˜ ì¡°ë¬¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return []

    if not jo_list:
        logging.warning(f"'{law_mst}'ì— ëŒ€í•œ ì¡°ë¬¸ ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return []

    # ----------------------------------------------------
    # â–¼â–¼â–¼ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•µì‹¬ ë¡œì§ â–¼â–¼â–¼
    # ----------------------------------------------------

    all_chunks = []
    # í•œ ë²ˆì— 8ëª…ì˜ ë³´ì¡° ì§ì›(ìŠ¤ë ˆë“œ)ì´ ë™ì‹œì— APIì— ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤.
    # ìˆ«ìë¥¼ ëŠ˜ë¦¬ë©´ ë” ë¹¨ë¼ì§ˆ ìˆ˜ ìˆì§€ë§Œ, ì„œë²„ì— ë¶€ë‹´ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 8 ì •ë„ê°€ ì ë‹¹í•©ë‹ˆë‹¤.
    with ThreadPoolExecutor(max_workers=8) as executor:

        future_to_jo = {
            executor.submit(fetch_single_article_detail, law_mst, jo_item.get("ì¡°ë¬¸ë²ˆí˜¸")): jo_item
            for jo_item in jo_list
        }

        for future in as_completed(future_to_jo):
            jo_item = future_to_jo[future]
            try:
                article_detail = future.result()
                if article_detail:
                    text_for_embedding = flatten_api_response(article_detail)
                    chunk_data = {
                        "ì¡°ë¬¸ë²ˆí˜¸": jo_item.get("ì¡°ë¬¸ë²ˆí˜¸", ""),
                        "ì¡°ë¬¸ì œëª©": article_detail.get("ì¡°ë¬¸ì œëª©", ""),
                        "ì¡°ë¬¸ë‚´ìš©": article_detail, # ì›ë³¸ JSON ë°ì´í„°
                        "text_for_embedding": text_for_embedding, # ê°€ê³µëœ í…ìŠ¤íŠ¸
                    }
                    all_chunks.append(chunk_data)
            except Exception as exc:
                jo_no = jo_item.get("ì¡°ë¬¸ë²ˆí˜¸", "ì•Œ ìˆ˜ ì—†ìŒ")
                logging.error(f"'{law_mst}'ì˜ ì¡°ë¬¸ '{jo_no}' ìƒì„¸ ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {exc}")

    logging.info(f"ì„±ê³µì ìœ¼ë¡œ ë²•ë ¹ ë³¸ë¬¸ì„ íŒŒì‹±í–ˆìŠµë‹ˆë‹¤. (MST: {law_mst}, ì´ ì¡°ë¬¸ ìˆ˜: {len(all_chunks)})")
    return all_chunks


def fetch_single_article_detail(law_mst: str, jo_no: str) -> Optional[Dict[str, Any]]:
    """
    ë‹¨ì¼ ì¡°ë¬¸ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ë¥¼ APIë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤. (ë³‘ë ¬ ì²˜ë¦¬ë  ì‘ì—…)
    """
    if not jo_no:
        return None

    formatted_jo_no = format_jo_no(jo_no)
    detail_api_url = (
        f"http://www.law.go.kr/DRF/lawService.do?OC={LAW_API_OC}&target=law"
        f"&MST={law_mst}&JO={formatted_jo_no}&type=JSON"
    )

    try:
        response = requests.get(detail_api_url, timeout=15)
        response.raise_for_status()
        detail_data = response.json()

        # API ì‘ë‹µì—ì„œ ì‹¤ì œ ì¡°ë¬¸ ìƒì„¸ ì •ë³´ê°€ ìˆëŠ” 'ë²•ë ¹' ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        article_detail = detail_data.get("ë²•ë ¹", {}).get("ì¡°ë¬¸", [{}])[0]
        return article_detail

    except (requests.RequestException, json.JSONDecodeError) as e:
        logging.warning(f"ì¡°ë¬¸({jo_no}) ìƒì„¸ ì •ë³´ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return None


def build_and_save_index(limit: int = None):
    logging.info("'laws' ë””ë ‰í„°ë¦¬ì—ì„œ ë²•ë ¹ë³„ ê°œë³„ DB ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    law_files = [os.path.join("laws", f) for f in os.listdir("laws") if f.endswith(".json")]
    total_files = len(law_files)
    processed_files_count = 0

    for law_file_path in law_files:
        if limit is not None and processed_files_count >= limit:
            logging.info(f"Limit of {limit} files reached. Stopping processing.")
            break

        processed_files_count += 1
        processed_files_count += 1
# â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
# base_filename = os.path.splitext(os.path.basename(law_file_path))[0] # ê¸°ì¡´ ì½”ë“œ ì£¼ì„ ì²˜ë¦¬ ë˜ëŠ” ì‚­ì œ
        logging.info(f"({processed_files_count}/{total_files}) '{os.path.basename(law_file_path)}' íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...")
# â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
        current_knowledge_base = {}
        current_texts_to_embed_map = {}

        try:
            with open(law_file_path, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
        except (IOError, json.JSONDecodeError) as e:
            logging.error(f"'{law_file_path}' íŒŒì¼ ë¡œë“œ ë˜ëŠ” íŒŒì‹± ì‹¤íŒ¨: {e}")
            continue

        law_items = raw_data.get("LawSearch", {}).get("law", [])
        if not isinstance(law_items, list):
            law_items = [law_items]

        if not law_items: continue
        item = law_items[0]

        law_mst = item.get("ë²•ë ¹ì¼ë ¨ë²ˆí˜¸")
        law_id = item.get("ë²•ë ¹ID") # ë²•ë ¹ìƒì„¸ë§í¬ ìƒì„±ì„ ìœ„í•´ ë²•ë ¹ID ì‚¬ìš©
        if not law_mst or not law_id: continue

        article_chunks = fetch_law_details(law_mst)

        for chunk in article_chunks:
            unique_id = f"{law_mst}-{chunk['ì¡°ë¬¸ë²ˆí˜¸']}"
            current_knowledge_base[unique_id] = {
                "ë²•ë ¹ëª…í•œê¸€": item.get("ë²•ë ¹ëª…í•œê¸€", ""),
                "ë²•ë ¹ìƒì„¸ë§í¬": f"http://www.law.go.kr/LSW/lsInfoP.do?lsiSeq={law_id}",
                "ì¡°ë¬¸ë²ˆí˜¸": chunk['ì¡°ë¬¸ë²ˆí˜¸'],
                "ì¡°ë¬¸ì œëª©": chunk['ì¡°ë¬¸ì œëª©'],
                "ì¡°ë¬¸ë‚´ìš©": chunk['text_for_embedding']
            }
            current_texts_to_embed_map[unique_id] = chunk['text_for_embedding']

        if not current_knowledge_base:
            logging.warning(f"'{os.path.basename(law_file_path)}'ì—ì„œ ì²˜ë¦¬í•  ì¡°ë¬¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
        # ê³ ìœ í•œ ì¶œë ¥ íŒŒì¼ ì´ë¦„ ìƒì„± (í•œê¸€ ëŒ€ì‹  ë²•ë ¹ì¼ë ¨ë²ˆí˜¸ ì‚¬ìš©)
        base_filename = law_mst  # ì˜ˆ: "272927"
        output_kb_file = f"{base_filename}_answers.json"
        output_index_file = f"{base_filename}_faiss_index.bin"
        output_id_map_file = f"{base_filename}_faiss_id_map.json"
        logging.info(f"ì´ {len(current_knowledge_base)}ê°œ ì¡°ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ. '{output_kb_file}' íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.")
        with open(output_kb_file, 'w', encoding='utf-8') as f:
            json.dump(current_knowledge_base, f, ensure_ascii=False, indent=4)
        # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²    

        logging.info(f"'{base_filename}'ì— ëŒ€í•œ Faiss ì¸ë±ìŠ¤ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        texts_to_embed = list(current_texts_to_embed_map.values())
        if not texts_to_embed: continue

        faiss_ids = list(range(len(texts_to_embed)))
        temp_id_map = {i: k for i, k in enumerate(current_texts_to_embed_map.keys())}

        embeddings = genai.embed_content(model=EMBEDDING_MODEL, content=texts_to_embed, task_type="RETRIEVAL_DOCUMENT")['embedding']
        embeddings_np = np.array(embeddings, dtype='float32')
        d = embeddings_np.shape[1]
        new_index = faiss.IndexFlatL2(d)
        new_index = faiss.IndexIDMap(new_index)
        new_index.add_with_ids(embeddings_np, np.array(faiss_ids))

        faiss.write_index(new_index, output_index_file)
        logging.info(f"Faiss ì¸ë±ìŠ¤ë¥¼ '{output_index_file}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

        with open(output_id_map_file, 'w', encoding='utf-8') as f:
            json.dump(temp_id_map, f)
        logging.info(f"ID ë§µì„ '{output_id_map_file}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    logging.info("ëª¨ë“  ë²•ë ¹ íŒŒì¼ì— ëŒ€í•œ ê°œë³„ DB ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# ==================================================================================
# â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² í•µì‹¬ ìˆ˜ì • ì˜ì—­ END â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
# ==================================================================================

def get_embedding(text: str, task_type="RETRIEVAL_DOCUMENT") -> list[float]:
    """Gemini APIë¥¼ í˜¸ì¶œí•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if task_type == "RETRIEVAL_DOCUMENT":
        title = "ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ê´€ë ¨ ì¡°í•­" if "ì‚°ì—…ì•ˆì „" in text else "ë²•ë¥  ì¡°í•­"
        result = genai.embed_content(
            model=EMBEDDING_MODEL,
            content=text,
            title=title,
            task_type=task_type)
    else:
        result = genai.embed_content(
            model=EMBEDDING_MODEL,
            content=text,
            task_type=task_type)
    return result['embedding']

def load_index():
    """
    ì„œë²„ ì‹œì‘ ì‹œ í˜¸ì¶œë  í•¨ìˆ˜.
    FAISS ì¸ë±ìŠ¤ì™€ ì›ë³¸ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤.
    """
    global index, id_map, knowledge_base
    required_files = [FAISS_INDEX_FILE, ID_MAP_FILE, KNOWLEDGE_BASE_FILE]
    for f in required_files:
        if not os.path.exists(f):
            logging.error(f"'{f}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. build_db.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ DBë¥¼ ìƒì„±í•˜ì„¸ìš”.")
            return

    logging.info(f"'{FAISS_INDEX_FILE}'ì—ì„œ ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
    index = faiss.read_index(FAISS_INDEX_FILE)

    logging.info(f"'{ID_MAP_FILE}'ì—ì„œ IDë§µì„ ë¡œë“œí•©ë‹ˆë‹¤.")
    with open(ID_MAP_FILE, 'r', encoding='utf-8') as f:
        id_map = {int(k): v for k, v in json.load(f).items()}

    logging.info(f"'{KNOWLEDGE_BASE_FILE}'ì—ì„œ knowledge baseë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
    with open(KNOWLEDGE_BASE_FILE, 'r', encoding='utf-8') as f:
        knowledge_base = json.load(f)

def search_vectors(query: str, k: int = 5) -> List[Dict[str, Any]]:
    """ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ì„ë² ë”©í•˜ê³  ë¯¸ë¦¬ ë¡œë“œëœ Faiss ì¸ë±ìŠ¤ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ kê°œì˜ ê²°ê³¼ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    if index is None:
        logging.error("ì¸ë±ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return []

    query_embedding = get_embedding(query, task_type="RETRIEVAL_QUERY")
    query_vector = np.array([query_embedding], dtype='float32')

    distances, indices = index.search(query_vector, k)

    results = []
    for i in range(len(indices[0])):
        if indices[0][i] == -1:
            continue

        faiss_id = indices[0][i]
        original_id = id_map.get(faiss_id)

        if original_id and original_id in knowledge_base:
            retrieved_data = knowledge_base[original_id]
            results.append({
                "id": original_id,
                "score": float(distances[0][i]),
                "data": retrieved_data
            })

    return results
