import requests
import json
import os
import re
import time
import logging
from typing import List, Dict, Optional, Any, Set

# [TF-IDF ë„ì…] í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

from dotenv import load_dotenv # ğŸ‘ˆ 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸

# ğŸ‘ˆ 2. FastAPI ì•±ì„ ìƒì„±í•˜ê¸° ì „ì— ê°€ì¥ ë¨¼ì € .env íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
#    ì´ë ‡ê²Œ í•˜ë©´ ì´ íŒŒì¼ë¿ë§Œ ì•„ë‹ˆë¼, ì—¬ê¸°ì„œ ì„í¬íŠ¸í•˜ëŠ” ëª¨ë“  ë‹¤ë¥¸ íŒŒì¼(vector_search_service ë“±)ì—ì„œë„
#    API í‚¤ë¥¼ ì •ìƒì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
load_dotenv()

# --- 1. ì„¤ì • ë° ìƒìˆ˜ ì •ì˜ ---

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# íŒŒì¼ ë° API ê´€ë ¨ ìƒìˆ˜
KNOWLEDGE_BASE_FILE = "answers.json"
OC_VALUE = 'shg30335'  # ìœ íš¨í•œ API í‚¤
SEARCH_URL = 'http://www.law.go.kr/DRF/lawSearch.do'
CONTENT_URL = 'http://www.law.go.kr/DRF/lawService.do'

# í•µì‹¬ ë²•ë ¹ ì„¸íŠ¸
CORE_LAW_SET = [
    "ì‚°ì—…ì•ˆì „ë³´ê±´ë²•", "ì¤‘ëŒ€ì¬í•´ ì²˜ë²Œ ë“±ì— ê´€í•œ ë²•ë¥ ", "ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì‹œí–‰ë ¹",
    "ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì‹œí–‰ê·œì¹™", "ì‚°ì—…ì•ˆì „ë³´ê±´ê¸°ì¤€ì— ê´€í•œ ê·œì¹™", "ì¤‘ëŒ€ì¬í•´ ì²˜ë²Œ ë“±ì— ê´€í•œ ë²•ë¥  ì‹œí–‰ë ¹",
    "ìœ„í—˜ë¬¼ì•ˆì „ê´€ë¦¬ë²•", "ì•¡í™”ì„ìœ ê°€ìŠ¤ì˜ ì•ˆì „ê´€ë¦¬ ë° ì‚¬ì—…ë²•", "ë„ì‹œê°€ìŠ¤ì‚¬ì—…ë²•",
    "ê±´ì„¤ê¸°ìˆ  ì§„í¥ë²•", "ê±´ì¶•ë²•", "ê±´ì„¤ê¸°ê³„ê´€ë¦¬ë²•", "ì‚°ì—…ì¬í•´ë³´ìƒë³´í—˜ë²•"
]

# --- [í•µì‹¬ ìˆ˜ì •] TfidfVectorizerê°€ ì‚¬ìš©í•  ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ ì •ì˜ ---
STOPWORDS: Set[str] = {
    'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ì—ê²Œ', 'ê»˜', 'í•œí…Œ',
    'ìœ¼ë¡œ', 'ë¡œ', 'ì™€', 'ê³¼', 'ê³ ', 'í•˜ë‹¤', 'ì´ë‹¤', 'ë˜ë‹¤'
}
# --- ìˆ˜ì • ì™„ë£Œ ---

# --- [í•µì‹¬ ìˆ˜ì •] TfidfVectorizer ìƒì„± ì‹œ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì „ë‹¬ ---
vectorizer = TfidfVectorizer(stop_words=list(STOPWORDS))
# --- ìˆ˜ì • ì™„ë£Œ ---

tfidf_matrix = None
kb_data_list = []

# --- 2. ë¡œì»¬ ì§€ì‹ ë² ì´ìŠ¤(JSON) ì²˜ë¦¬ í•¨ìˆ˜ ---

def load_and_prepare_knowledge_base(file_path: str):
    """[ìµœì¢… ìˆ˜ì •] ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ë¡œë“œí•˜ê³ , 'í‚¤ì›Œë“œ' í•„ë“œë¥¼ í¬í•¨í•˜ì—¬ TF-IDF ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤."""
    global tfidf_matrix, kb_data_list
    if not os.path.exists(file_path):
        logging.error(f"'{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        raise FileNotFoundError(f"ì˜¤ë¥˜: '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            kb_dict = json.load(f)
        
        kb_data_list = list(kb_dict.values())
        documents = []
        for item in kb_data_list:
            question_text = item.get("í•µì‹¬ì§ˆë¬¸", "")
            summary_text = " ".join(item.get("ì ìš©ìƒí™©", []))
            point_text = item.get("ì‹¤ë¬´í¬ì¸íŠ¸", "")
            keywords = " ".join(item.get("í‚¤ì›Œë“œ", []))
            search_corpus = f"{question_text} {summary_text} {point_text} {keywords} {keywords}"
            documents.append(search_corpus)

        tfidf_matrix = vectorizer.fit_transform(documents)
        logging.info(f"ì§€ì‹ ë² ì´ìŠ¤ ë¡œë”© ë° TF-IDF ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ. ì´ {len(kb_data_list)}ê°œ í•­ëª©.")
        return kb_dict
    except json.JSONDecodeError:
        logging.error(f"'{file_path}' íŒŒì¼ì˜ JSON í˜•ì‹ì´ ì˜¬ë°”ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        raise ValueError(f"ì˜¤ë¥˜: '{file_path}' íŒŒì¼ì˜ JSON í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")


def search_knowledge_base_tfidf(query: str) -> List[Dict[str, Any]]:
    """[ë‹¤ì¤‘ ë‹µë³€ ë°˜í™˜] TF-IDFì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ì í•©í•œ ë¬¸ì„œë“¤ì„ ì°¾ìŠµë‹ˆë‹¤."""
    global vectorizer, tfidf_matrix, kb_data_list
    results = []
    if not query.strip():
        return results
    query_vector = vectorizer.transform([query])
    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()
    related_docs_indices = cosine_similarities.argsort()[::-1]
    SIMILARITY_THRESHOLD = 0.1
    for i in related_docs_indices:
        if cosine_similarities[i] > SIMILARITY_THRESHOLD and len(results) < 3:
            match_item = kb_data_list[i].copy() # ì›ë³¸ ìˆ˜ì •ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë³µì‚¬ë³¸ ì‚¬ìš©
            match_item['similarity_score'] = cosine_similarities[i]
            results.append(match_item)
    if results:
        logging.info(f"ê²€ìƒ‰ ì™„ë£Œ. ì´ {len(results)}ê°œì˜ ìœ ì‚¬ í•­ëª©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    else:
        logging.warning(f"ê²€ìƒ‰ ê²°ê³¼, ì¼ì¹˜í•˜ëŠ” í•­ëª©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    return results

def format_answer(item: Dict[str, Any], rank: int) -> str:
    """ì°¾ì•„ë‚¸ ì§€ì‹ ë² ì´ìŠ¤ í•­ëª©ì„ ì§€ì •ëœ í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ ê°€ê³µí•©ë‹ˆë‹¤."""
    summary = "\n".join(f"- {line}" for line in item.get("ì ìš©ìƒí™©", ["ìš”ì•½ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."]))
    practical_point = item.get("ì‹¤ë¬´í¬ì¸íŠ¸", "ì‹¤ë¬´ í¬ì¸íŠ¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    score = item.get('similarity_score', 0)
    legal_basis_obj = item.get("ê¸°ë³¸ì¡°ë¬¸", {})
    if isinstance(legal_basis_obj, dict):
        law_name = legal_basis_obj.get('ë²•ë ¹ëª…', 'N/A')
        article = legal_basis_obj.get('ì¡°ë¬¸', 'N/A')
        legal_basis_str = f"**{law_name} {article}**"
    else:
        legal_basis_str = "ê´€ë ¨ ë²•ë ¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
    formatted_output = f"""
### Kandidat {rank} (ìœ ì‚¬ë„: {score:.2f}) - {item.get('í•µì‹¬ì§ˆë¬¸', 'N/A')}
---
#### ğŸ’¡ ì ìš© ìƒí™©
{summary}
#### âš–ï¸ ë²•ë ¹ ê·¼ê±°
{legal_basis_str}
#### âœ… í˜„ì¥ ì‹¤ë¬´ í¬ì¸íŠ¸
{practical_point}
"""
    return formatted_output.strip()

# --- 3. ë²•ë ¹ API ì¡°íšŒ(lawFetcher) ê´€ë ¨ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---

def search_law(law_name):
    """ë²•ë ¹ëª…ìœ¼ë¡œ êµ­ê°€ë²•ë ¹ì •ë³´ DRF APIì—ì„œ JSON ë°ì´í„°ë¥¼ ì¡°íšŒ"""
    base_url = "http://www.law.go.kr/DRF/lawSearch.do"
    oc_key = os.getenv("LAW_API_OC")  # .envì— ë“±ë¡í•œ ê¸°ê´€ì½”ë“œ
    print(f"ë²•ë ¹ API í˜¸ì¶œ: {law_name} (ê¸°ê´€ì½”ë“œ: {oc_key})")
    params = {
        "OC": oc_key,
        "target": "law",
        "type": "JSON",
        "query": law_name
    }
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
    }
    try:
        resp = requests.get(base_url, params=params, headers=headers, timeout=10)
        print(f"API ìš”ì²­ URL: {resp.url}")  # ìš”ì²­ URL ì¶œë ¥
        if resp.status_code == 200 and "application/json" in resp.headers.get("Content-Type", ""):
            print(f"âœ… '{law_name}' ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ")
            return resp.json()
        else:
            print(f"âŒ '{law_name}' ì¡°íšŒ ì‹¤íŒ¨ (ìƒíƒœ:{resp.status_code}) â†’ ì‘ë‹µ íƒ€ì…: {resp.headers.get('Content-Type')}")
            return None
    except Exception as e:
        print(f"âŒ '{law_name}' ì¡°íšŒ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return None


# --- 4. ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
def run_local_search():
    """ë¡œì»¬ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ëŠ” ëª¨ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    question = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
    logging.info(f"ì‚¬ìš©ì ì§ˆë¬¸: \"{question}\"")
    found_items = search_knowledge_base_tfidf(question)
    print("\n" + "="*40)
    if found_items:
        print(f"âœ… AI ì–´ì‹œìŠ¤í„´íŠ¸ ë‹µë³€ì…ë‹ˆë‹¤. (ì´ {len(found_items)}ê°œ)")
        for i, item in enumerate(found_items):
            final_answer = format_answer(item, i + 1)
            print(final_answer)
            print("\n" + "-"*40)
        print("ğŸ“š ì¶œì²˜: ë‚´ë¶€ ì „ë¬¸ê°€ ì§€ì‹ ë² ì´ìŠ¤ (answers.json)")
    else:
        print("âš ï¸ ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("="*40)

def run_api_fetcher():
    print("==============================================")
    print("     ì‚°ì—…ì•ˆì „ AI - í•µì‹¬ ë²•ë ¹ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì‹œì‘")
    print("==============================================")

    law_database = {}

    for law_name in CORE_LAW_SET:
        law_content = search_law(law_name)
        if law_content:
            law_database[law_name] = law_content
        time.sleep(0.5)
        print("-" * 50)

    print("\n\n==============================================")
    print("     âœ… í•µì‹¬ ë²•ë ¹ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
    print(f"     ì´ {len(law_database)}ê°œì˜ ë²•ë ¹ ì •ë³´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
    print("     ì €ì¥ëœ ë²•ë ¹ ëª©ë¡:", list(law_database.keys()))
    print("==============================================")
    
    # law_databaseë¥¼ íŒŒì¼ë¡œ ì €ì¥
    with open("law_database.json", "w", encoding="utf-8") as f:
        json.dump(law_database, f, ensure_ascii=False, indent=2)
        print("ğŸ“ law_database.json íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ")

    # ì˜ˆì‹œ: ì‚°ì—…ì•ˆì „ë³´ê±´ë²•ì˜ ì´ì¡°ë¬¸ìˆ˜ ì¶œë ¥
    if "ì‚°ì—…ì•ˆì „ë³´ê±´ë²•" in law_database:
        san_an_bub = law_database["ì‚°ì—…ì•ˆì „ë³´ê±´ë²•"]
        if "law" in san_an_bub and "ì´ì¡°ë¬¸ìˆ˜" in san_an_bub["law"]:
            print(f"\nì°¸ê³ : 'ì‚°ì—…ì•ˆì „ë³´ê±´ë²•'ì€ ì´ {san_an_bub['law']['ì´ì¡°ë¬¸ìˆ˜']}ê°œì˜ ì¡°ë¬¸ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        load_and_prepare_knowledge_base(KNOWLEDGE_BASE_FILE)
        while True:
            print("\nì–´ë–¤ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
            mode = input("1: ë¡œì»¬ ì§ˆë¬¸ ê²€ìƒ‰, 2: ë²•ë ¹ API ë°ì´í„° êµ¬ì¶•, 3: ì¢…ë£Œ (ìˆ«ì ì…ë ¥): ")
            if mode == '1':
                run_local_search()
            elif mode == '2':
                run_api_fetcher()
            elif mode == '3':
                break
            else:
                print("ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. 1, 2, 3 ì¤‘ í•˜ë‚˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    except (FileNotFoundError, ValueError) as e:
        print(f"\n[ì‹¤í–‰ ì˜¤ë¥˜] í”„ë¡œê·¸ë¨ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({e})")
    except (KeyboardInterrupt, EOFError):
        print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()